# ğŸ—ï¸ Data Warehouse Solution on AWS

This project implements an **end-to-end serverless data warehouse solution** on AWS.  
It was built after completing the AWS training *"Building Data Warehouse Solution"* and extended with automation, orchestration, and analytics queries.

---

## ğŸš€ Project Overview

The pipeline ingests raw sales data from Kaggle, cleans and transforms it using AWS Glue, and loads curated fact/dimension tables into **Amazon Redshift Serverless**.  
It also includes automation with AWS Lambda, EventBridge, and S3 triggers to orchestrate the entire workflow.

---

## ğŸ”„ Data Pipeline Flow

```mermaid
flowchart TD
    A[Amazon S3\nRaw Data] -->|Trigger Glue Job 1| B[AWS Glue\nClean & Transform]
    B -->|Output Parquet â†’ transformed/| C[AWS Lambda 2]
    C -->|Trigger Glue Job 2| D[AWS Glue\nSplit Fact & Dim]
    D -->|Output Parquet â†’ curated/| E[AWS Lambda 3]
    E -->|COPY| F[Amazon Redshift Serverless\nFact & Dimension Tables]
    F -->|Analytics Queries| G[Power BI]
    
    subgraph Automation
        H[Amazon EventBridge\nEvery 10 min] -->|Invoke| I[AWS Lambda 1]
        I -->|Trigger Glue Job 1| B
    end
```

---

## ğŸ“‚ Project Structure

### ğŸ“ dataset
- `amazon_products_sales_data_uncleaned.csv` â†’ raw dataset (downloaded from Kaggle)

### ğŸ“ scripts
- **automation/**
  - `pipeline_automation.py` â†’ Configure S3 â†’ Lambda 2 â†’ Lambda 3 triggers
- **cleanup/**
  - `delete_resources.py` â†’ Tear down all AWS resources
- **eventbridge/**
  - `eventbridge.py` â†’ Create schedule (every 10 min â†’ Lambda 1)
- **glue/**
  - `create_glue_job.py` â†’ Register Glue jobs
  - `glue_clean_transform.py` â†’ Clean & transform raw CSV â†’ Parquet
  - `glue_split_fact_dim.py` â†’ Split into fact & dimension parquet tables
- **iam/**
  - `create_iam_roles.py` â†’ Provision IAM roles for Glue, Lambda, Redshift
- **lambda/**
  - `deploy_lambdas.py` â†’ Package & deploy Lambda 1/2/3
  - `lambda_1.py` â†’ Start Glue Job 1 (clean + transform)
  - `lambda_2.py` â†’ Start Glue Job 2 (split fact/dim)
  - `lambda_3.py` â†’ Provision Redshift & load curated data
- **redshift/**
  - `create_redshift_serverless.py` â†’ Setup namespace, workgroup, BI user
- **s3/**
  - `create_bucket_and_folders.py` â†’ Create S3 bucket & folder structure
  - `upload_raw_data.py` â†’ Ingest dataset â†’ S3 raw/

### ğŸ“„ Root Files
- `redshift_analysis_queries.sql` â†’ SQL queries for insights (10 use cases)  
- `requirements.txt` â†’ Python dependencies

---

## âš™ï¸ Workflow

1. **Ingestion** â†’ Kaggle dataset uploaded to S3 (`raw/`).
2. **Glue ETL** â†’ Job 1 cleans & transforms â†’ `transformed/`; Job 2 splits â†’ `curated/`.
3. **Automation** â†’ EventBridge triggers Lambda 1; S3 triggers Lambda 2; Glue job triggers Lambda 3.
4. **Data Warehouse** â†’ Curated data loaded into Redshift tables (`fact_sales`, `dim_product`, `dim_date`).
5. **Analysis Layer** â†’ 10 SQL queries provide insights (popularity, pricing, trends). Dashboards can be built in **Power BI**.
6. **Cleanup** â†’ `delete_resources.py` deletes all AWS resources.

---

## ğŸ“Š Redshift Schema

- **dim_product**  
  Distribution: `DISTSTYLE ALL`  
  ```sql
  product_name VARCHAR(255),
  category VARCHAR(255)
  ```

- **dim_date**  
  Distribution: `DISTSTYLE ALL`  
  ```sql
  crawl_year INT,
  crawl_month INT
  ```

- **fact_sales**  
  Distribution: `DISTSTYLE KEY (product_name)`  
  Sort Key: `(crawl_year, crawl_month)`  
  ```sql
  product_name,
  current_discounted_price,
  listed_price,
  rating,
  number_of_reviews,
  discount_amount,
  discount_flag,
  rating_bucket,
  crawl_year,
  crawl_month
  ```

---

## ğŸ”‘ Key Learnings

- Automating **ETL â†’ Data Warehouse â†’ BI** on AWS with serverless services  
- Secure cross-service access with **IAM roles**  
- Event-driven orchestration with **EventBridge, S3 triggers, and Lambdas**  
- Designing **fact and dimension models** in Redshift  
- Writing analytical SQL for business insights

---

## ğŸ§¹ Cleanup

Run:
```bash
python scripts/cleanup/delete_resources.py
```
This removes all AWS resources provisioned for the project.

---

## ğŸ› ï¸ Requirements

- Python 3.9+  
- boto3, botocore, awscli  
- AWS account with IAM permissions  
- (Optional) KaggleHub for dataset ingestion

Install dependencies:
```bash
pip install -r requirements.txt
```

---

## ğŸ“Œ Next Steps

- Build **Power BI dashboards** for visualization  
- Store Redshift credentials in **AWS Secrets Manager**  
- Scale Glue/Redshift configs for larger datasets

---

## ğŸ™Œ Acknowledgment

This project was created after completing AWSâ€™s training:  
**_"Building Data Warehouse Solution"_**